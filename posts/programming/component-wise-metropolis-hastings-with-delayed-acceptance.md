---
title:
published:
tags: 機械学習, ベイズ推論, サンプリングアルゴリズム
toc: on
mathjax: on
---

汎用的なサンプリングアルゴリズムであるメトロポリス・ヘイスティングス法について解説します. ただし数学的な証明等は行わず, 事実を述べるだけにします.

<!--more-->


## サンプリング手法の必要性

ベイズ推論において事後分布が解析的に計算できない場面というのは往々にして存在します. その際のひとつの解決策がサンプリングです.

つまり, 事後分布に従うサンプルを得ることで, 事後分布について分かったことにしようということです.


## サンプリング手法の種類


## メトロポリス・ヘイスティングス法

文章中ではめんどいのでMH法と書くことにします.

### 概要

まずは記号の設定をします.

- $\pi(x)$: 知りたい分布の確率密度関数. 正規化係数のみ計算できない.
- $\pi_*(x)$: $\pi(x)$ の正規化係数を除いた部分.

ベイズ推論において, 事後分布の正規化係数のみ分からないという場面はよく出てきます. したがってMH法をベイズ推論において使う場合, 正規化係数以外は計算できるという条件は足かせにはなりません.

MH法は以下のステップからなります.

- $x_t$ まで得られたとする.
- 提案分布 $q(x \mid x_t)$ から $x_{t+1}$ の候補 $x_*$ をサンプルする.
- $\alpha(x_t, x_*)$ の確率で $x_*$ を受理し, $x_{t+1} = x_*$ とする.
- $x_*$ が棄却されたら $x_{t+1} = x_t$ とする.

<!--
ただし提案分布 $q(x \mid x_t)$ は次の詳細釣り合い条件 (detaild balance condition) を満たすものとします.

$$
\pi(x)q(y \mid x) = \pi(y)q(x \mid y)
$$
-->

ただし受理確率 $\alpha(x_t, x_*)$ は以下で定義されます.

$$
\alpha(x_t, x_*) = \min\left\{\frac{\pi(x_*)q(x_t \mid x_*)}{\pi(x_t)q(x_* \mid x_t)}, 1\right\}
$$

$\pi$ と $\pi_*$ は定数倍違うだけなので, 上の式の $\pi$ の代わりに $\pi_*$ を使うことができます. ここで, 正規化係数は分かっていなくてもいいというのが効いてきます.

以上の一連のステップにより, $x_0, x_1, \ldots$ はマルコフ連鎖をなし, 分布 $\pi(x)$ を定常分布として持ちます.


### 具体例
具体的な分布を用いて実験してみましょう. まず知りたい分布はガンマ分布とします. 今回は正規化係数がきれいな形をしているので, $\pi(x)$ をそのまま使えますね.

$$
\pi(x) = \mathrm{Gam}(x \mid 2, 2) = xe^{-2x}
$$

提案分布は, 直前のサンプルを中心とする正規分布にしましょう.

$$
q(x_* \mid x_t) = \mathcal{N}(x_* \mid x_t, 1.0)
$$

すると $q(x_* \mid x_t) = q(x_t \mid x_*)$ を満たすので, 受理確率は以下のように簡単になります.

$$
\alpha(x_t, x_*) = \min\left\{\frac{\pi(x_*)}{\pi(x_t)}, 1\right\}
$$

julia で書けば以下のようになるでしょう.

```julia
using Random

burn_in = 100
iter = 1000

# ガンマ分布
gam(x) = x * exp(-2.0 * x)

# 受理確率 (rand() と比較するため, min{-, 1} というふうに考えなくてよい)
α(x_t, x_*) = gam(x_*) / gam(x_t)

# 初期値
x = 1.0
samples = Vector{Float64}()

for _ = 1:burn_in
    x = next_sample(x)
end

for _ = 1:iter
    x = next_sample(x)
    push!(samples, x)
end

function next_sample(x)
    # x を中心とする分散が1であるような正規分布から x_* を取得
    x_* = randn() + x
    acceptance_rate = α(x, x_*)

    if acceptance_rate < rand(x)
        return x
    else
        return x_*
    end
end
```

### どんなケースで使うのか？

ベイズ推論の文脈で言えば, 事後分布が解析的に計算できない場合に, サンプリング手法に力を借りるという選択肢が浮上します.

たとえば, [ベイズ推論による機械学習](https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E5%85%A5%E9%96%80-KS%E6%83%85%E5%A0%B1%E7%A7%91%E5%AD%A6%E5%B0%82%E9%96%80%E6%9B%B8-%E9%A0%88%E5%B1%B1-%E6%95%A6%E5%BF%97/dp/4061538322/ref=sr_1_1?__mk_ja_JP=%E3%82%AB%E3%82%BF%E3%82%AB%E3%83%8A&keywords=%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96&qid=1575947974&sr=8-1) ではMH法の1種であるギブスサンプリングが紹介されています.

### 課題

理論的には上記の方法でサンプリングが可能です. しかしパラメータの数が多いと棄却率が高くなり, かなり多くのサンプルが必要となってしまいます.

提案分布として, 直前のサンプルを中心とした高次元正規分布を採用したとしましょう. この場合は提案分布である正規分布の分散を小さくすれば, 棄却率を下げることができます.

しかしサンプリング1回あたりに進める距離が小さくなるため, 結局多くのサンプルが必要になってしまいます.

### 成分ごとにサンプリング

上の問題の原因は, 高次元であることです. 受理確率が低くなるのは次元の呪いを受けてしまっているからです.

したがってこの問題の解決策として, 成分ごとにサンプリングする方法が考えられます.

変数を $N$ 個持つモデルを考えているとしたら, 以下のようなアルゴリズムになるでしょう.

- $x_{1, 0}, x_{2, 0}, \ldots, x_{N, 0}$ を初期化.
- 提案分布 $q_1(x \mid x_{1, 0})$ から $x_{1, 1}$ の候補 $x_{1, *}$ を取得.
- 受理確率 $\alpha_1(x_{1, 0}, x_{1, *})$ により 受理/棄却 を決定.
- 中略.
- 提案分布 $q_k(x \mid x_{k, 0})$ から $x_{k, 1}$ の候補 $x_{k, *}$ を取得.
- 受理確率 $\alpha_k(x_{k, 0}, x_{k, *})$ により 受理/棄却 を決定.
- これを $N$ まで行う.
- これを繰り返す.

ただし受理確率は以下の通りです.

$$
\alpha_k(x_{k, t}, x_{k, *}) = \min\left\{
    \frac{
        \pi(x_{k, *} \mid x_{1, t+1}, \ldots, x_{k-1, t+1}, x_{k+1, t}, \ldots, x_{N, t})q_k(x_{k, t} \mid x_{k, *})
    }{
        \pi(x_{k, t} \mid x_{1, t+1}, \ldots, x_{k-1, t+1}, x_{k+1, t}, \ldots, x_{N, t})q_k(x_{k, *} \mid x_{k, t})
    }
\right\}
$$

しかし全ての成分を個別に扱うことで, 実行時間が非現実的なほどまで増大することもありえます. そのような場合は変数をグルーピングし, そのグループごとにサンプリングするという工夫ができます.

### ギブスサンプリングとの違い

ギブスサンプリングは成分ごとのMH法の1種です. ギブスサンプリングは, 条件付き分布 $\pi(x_k \mid x_1, \ldots, x_{k-1}, x_{k+1}, \ldots x_{N})$ が既知である場合に適用できるアルゴリズムです.

これが既知である場合は, わざわざよそから提案分布をもってくる必要がありません. この条件付き分布からサンプルすればいいからです.

すると受理確率は常に $1$ となり, 棄却されることはありません.

### 早期リジェクトで高速化

高次元の呪いを解くためには成分ごとにサンプリングすればいいと述べました. しかしそれでは実行時間が大幅に増加してしまいます.

それゆえ変数をグループ化し, グループごとにサンプリングするという方法も考えられるのでした.

しかしそれだけではなかなか計算量を減らすことができません. ここでは数学的な方面から計算量を減らす方法を解説します.

記号が複雑になるので, 変数をまるごとサンプリングする場合を考えます(もしくは1変数の場合を考えていると思っていただいても構いません).
